{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7TC6FIl90o2"
   },
   "source": [
    "# Задание 4 - Перенос обучения (transfer learning) и тонкая настройка (fine-tuning)\n",
    "\n",
    "Одной из важнейшних техник в тренировке сетей - использовать заранее натренированные веса на более общей задачи в качестве начальной точки, а потом \"дотренировать\" их на конкретной.\n",
    "\n",
    "Такой подход и убыстряет обучение, и позволяет тренировать эффективные модели на маленьких наборах данных.\n",
    "\n",
    "В этом упражнении мы натренируем классификатор, который отличает хотдоги от не хотдогов!  \n",
    "(более подробно - https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "\n",
    "Это задание требует доступа к GPU, поэтому его можно выполнять либо на компьютере с GPU от NVidia, либо в [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcXBeP1O7cnY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "from socket import timeout\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "import re\n",
    "from skimage import io, transform\n",
    "\n",
    "!pip3 install -q torch torchvision\n",
    "#!pip3 install -q Pillow==4.0.0\n",
    "!pip3 install -q Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nfl0A9X90pK"
   },
   "source": [
    "Сначала давайте скачаем данные с картинками. Это сделает код в следующей ячейке. Данные будут разделены на две части. На обучающей выборке, которая будет храниться в папке **train_kaggle**, мы будем строить наши модели, а на тестовой выборке **test_kaggle** будем предсказывать класс, к которому относится фотография (хотдог или нет).\n",
    "\n",
    "### Если вы в Google Colab!\n",
    "\n",
    "В нем можно запускать ноутбуки с доступом к GPU. Они не очень быстрые, зато бесплатные!\n",
    "Каждый ноутбук получает свой собственный environment c доступным диском итд.\n",
    "\n",
    "Через 90 минут отсуствия активности этот environment пропадает со всеми данными.\n",
    "Поэтому нам придется скачивать данные каждый раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ourBj07Arm3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-30 12:13:01--  https://storage.googleapis.com/dlcourse_ai/train.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.21.240, 172.217.23.176, 172.217.23.144, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.21.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 562348083 (536M) [application/zip]\n",
      "Saving to: 'train.zip'\n",
      "\n",
      "train.zip           100%[===================>] 536.30M  51.3MB/s    in 13s     \n",
      "\n",
      "2020-07-30 12:13:15 (40.4 MB/s) - 'train.zip' saved [562348083/562348083]\n",
      "\n",
      "Number of files in the train folder 4603\n",
      "--2020-07-30 12:13:19--  https://storage.googleapis.com/dlcourse_ai/test.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.21.208, 172.217.16.208, 172.217.22.80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.21.208|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 140788786 (134M) [application/zip]\n",
      "Saving to: 'test.zip'\n",
      "\n",
      "test.zip            100%[===================>] 134.27M  49.8MB/s    in 2.7s    \n",
      "\n",
      "2020-07-30 12:13:22 (49.8 MB/s) - 'test.zip' saved [140788786/140788786]\n",
      "\n",
      "Number of files in the test folder 1150\n"
     ]
    }
   ],
   "source": [
    "# Download train data\n",
    "!wget -nc \"https://storage.googleapis.com/dlcourse_ai/train.zip\"\n",
    "!unzip -qn \"train.zip\"\n",
    "\n",
    "train_folder = \"train_kaggle/\"\n",
    "# Count number of files in the train folder, should be 4603\n",
    "print('Number of files in the train folder', len(os.listdir(train_folder)))\n",
    "\n",
    "# Download test data\n",
    "!wget  -nc \"https://storage.googleapis.com/dlcourse_ai/test.zip\"\n",
    "!unzip -qn \"test.zip\"\n",
    "\n",
    "test_folder = \"test_kaggle/\"\n",
    "# Count number of files in the test folder, should be 1150\n",
    "print('Number of files in the test folder', len(os.listdir(test_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU-OD9O9ltP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch.nn.functional as tf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djlPWFLc90pb"
   },
   "source": [
    "# Имплементируем свой Dataset для загрузки данных\n",
    "\n",
    "В этом задании мы реализуем свой собственный класс Dataset для загрузки данных. Его цель - загрузить данные с диска и выдать по ним тензор с входом сети, меткой и идентификатором картинки (так будет проще подготовить сабмит для kaggle на тестовых данных).\n",
    "\n",
    "Вот ссылка, где хорошо объясняется как это делать на примере: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "Ваш Dataset должен в качестве количества сэмплов выдать количество файлов в папке и уметь выдавать кортеж из сэмпла, метки по индексу и названия файла.\n",
    "Если название файла начинается со слов 'frankfurter', 'chili-dog' или 'hotdog' - метка положительная. Иначе отрицательная (ноль).\n",
    "\n",
    "И не забудьте поддержать возможность трансформации входа (аргумент `transforms`), она нам понадобится!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bN2SPiJa9v5M"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "class HotdogOrNotDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        \n",
    "        # TODO: Your code here!\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.folder))\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        # TODO Implement getting item by index\n",
    "        # Hint: os.path.join is helpful!\n",
    "        img_name = os.listdir(self.folder)[index]\n",
    "        img_path = os.path.join(self.folder, img_name\n",
    "                                )\n",
    "        image = io.imread(img_path)\n",
    "        #print(re.match(r'frankfurter|chili-dog|hotdog', img_name), img_name)\n",
    "        if re.match(r'frankfurter|chili-dog|hotdog', img_name):\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "        #plt.imshow(image)\n",
    "        imagePIL = tvtf.ToPILImage()(image)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            imagePIL = self.transform(imagePIL)\n",
    "\n",
    "        return imagePIL, y, img_name\n",
    "\n",
    "def visualize_samples(dataset, indices, title=None, count=10, labels=None):\n",
    "    # visualize random 10 samples\n",
    "    fig1 = plt.figure(figsize=(count*3,3))\n",
    "    display_indices = indices[:count]\n",
    "    if title:\n",
    "        fig1.suptitle(\"%s %s/%s\" % (title, len(display_indices), len(indices)))        \n",
    "    for i, index in enumerate(display_indices):    \n",
    "        x, y, _ = dataset[index]\n",
    "        ax = fig1.add_subplot(1,count,i+1)\n",
    "        if labels:\n",
    "          ax.set_title(labels[i])  \n",
    "        else:\n",
    "          ax.set_title(\"Label: %s\" % y)\n",
    "        ax.imshow(x)\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')   \n",
    "    \n",
    "orig_dataset = HotdogOrNotDataset(train_folder)\n",
    "indices = np.random.choice(np.arange(len(orig_dataset)), 7, replace=False)\n",
    "\n",
    "visualize_samples(orig_dataset, indices, \"Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQNsUvYm4_2V"
   },
   "outputs": [],
   "source": [
    "# Let's make sure transforms work!\n",
    "dataset = HotdogOrNotDataset(train_folder, transform=transforms.RandomVerticalFlip(0.9))\n",
    "\n",
    "visualize_samples(dataset, indices, \"Samples with flip - a lot should be flipped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HquJWqCy90pq"
   },
   "source": [
    "# Создаем Dataset для тренировки\n",
    "\n",
    "И разделяем его на train и validation.\n",
    "На train будем обучать модель, на validation проверять ее качество, а соревнование Kaggle In-Class проведем на фотографиях из папки test_kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAvkoRx-9FsP"
   },
   "outputs": [],
   "source": [
    "# First, lets load the dataset\n",
    "train_dataset = HotdogOrNotDataset(train_folder, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ColorJitter(hue=.50, saturation=.50, \n",
    "                                                  brightness = .50, contrast = .50),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.RandomVerticalFlip(),\n",
    "                           #transforms.RandomCrop((224, 224)),\n",
    "                           transforms.RandomRotation(50, resample=Image.BILINEAR),\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )\n",
    "val_dataset = HotdogOrNotDataset(train_folder, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )\n",
    "test_dataset = HotdogOrNotDataset(test_folder, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRnr8CPg7Hli"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data_size = len(dataset)\n",
    "validation_fraction = .2\n",
    "\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "# Notice that we create test data loader in a different way. We don't have the labels.\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIUEN2uV90p1"
   },
   "source": [
    "Наши обычные функции для тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ek3KVQK7hJ6"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, scheduler=None,  num_epochs=10):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y,_) in enumerate(train_loader):\n",
    "          \n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)    \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value.item()\n",
    "            \n",
    "            del x_gpu, y_gpu, prediction, loss_value, indices\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy, val_loss, val_f1 = compute_accuracy(model, val_loader)\n",
    "        if scheduler:\n",
    "          scheduler.step(val_loss)\n",
    "          #scheduler.step()\n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "\n",
    "        \n",
    "        print(\"Epoch: %d, Average loss: %f, Train accuracy: %f, Val accuracy: %f, Val F1: %f\" % (epoch, ave_loss, train_accuracy, val_accuracy, val_f1))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "from scipy.special import softmax\n",
    "    \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
    "    #       and compute the overall accuracy.\n",
    "    # Hint: PyTorch has the argmax function!\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    loss_accum = 0\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for i_step, (x, y, _) in enumerate(loader):\n",
    "            #print(x.shape)\n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)\n",
    "            predictions.extend(torch.argmax(tf.softmax(prediction, dim=0), dim=1).cpu().detach().numpy()) \n",
    "            ground_truth.extend(y)\n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y_gpu.shape[0]\n",
    "            loss_value = loss(prediction.cpu(), y)\n",
    "            loss_accum += loss_value\n",
    "            #loss_accum += 0\n",
    "            del x_gpu, y_gpu, prediction, indices\n",
    "        accuracy = float(correct_samples) / total_samples\n",
    "        precision, recall, f1 = binary_classification_metrics(predictions, ground_truth)\n",
    "    return accuracy, loss_accum, f1\n",
    "    #return accuracy, loss_accum\n",
    "    # Don't forget to move the data to device before running it through the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33817,
     "status": "ok",
     "timestamp": 1595836744415,
     "user": {
      "displayName": "Nadya Strogankova",
      "photoUrl": "",
      "userId": "11793698916100822526"
     },
     "user_tz": -180
    },
    "id": "oxw1R5rZPTiF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import torch.nn.functional as tf\n",
    "\n",
    "class SubsetSampler(Sampler):\n",
    "    r\"\"\"Samples elements with given indices sequentially\n",
    "\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        indices (ndarray): indices of the samples to take\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    \n",
    "def evaluate_model(model, dataset, indices):\n",
    "    \"\"\"\n",
    "    Computes predictions and ground truth labels for the indices of the dataset\n",
    "    \n",
    "    Returns: \n",
    "    predictions: np array of booleans of model predictions\n",
    "    grount_truth: np array of boolean of actual labels of the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    val_sampler = SubsetSampler(indices)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=val_sampler)\n",
    "\n",
    "    model.eval() # Evaluation mode\n",
    "    \n",
    "    # TODO: Evaluate model on the list of indices and capture predictions\n",
    "    # and ground truth labels\n",
    "    # Hint: SubsetSampler above could be useful!\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for i_step, (x, y, _) in enumerate(loader):\n",
    "        #print(i_step)\n",
    "        x_gpu = x.to(device)\n",
    "        prediction = model(x_gpu) \n",
    "        _, indices = torch.max(prediction, 1)\n",
    "        ground_truth.extend(y.detach().numpy())\n",
    "        predictions.extend(torch.argmax(tf.softmax(prediction, dim=0), dim=1).cpu().detach().numpy())\n",
    "    \n",
    "    return predictions, ground_truth\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def binary_classification_metrics(prediction, ground_truth):\n",
    "    # TODO: Implement this function!\n",
    "    # We did this already it in the assignment1\n",
    "    precision = metrics.precision_score(ground_truth, prediction)\n",
    "    recall = metrics.recall_score(ground_truth, prediction)\n",
    "    f1 = metrics.f1_score(ground_truth, prediction)\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d28FkVbg90p7"
   },
   "source": [
    "# Использование заранее натренированной сети (pretrained network)\n",
    "\n",
    "Чаще всего в качестве заранее натренированной сети используется сеть, натренированная на данных ImageNet с 1M изображений и 1000 классами.\n",
    "\n",
    "PyTorch включает такие натренированные сети для различных архитектур (https://pytorch.org/docs/stable/torchvision/models.html)  \n",
    "Мы будем использовать ResNet18.\n",
    "\n",
    "Для начала посмотрим, что выдает уже натренированная сеть на наших картинках. То есть, посмотрим к какому из 1000 классов их отнесет сеть.\n",
    "\n",
    "Запустите модель на 10 случайных картинках из датасета и выведите их вместе с классами с наибольшей вероятностью.  \n",
    "В коде уже есть код, который формирует соответствие между индексами в выходном векторе и классами ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0YXUZOWBZor"
   },
   "outputs": [],
   "source": [
    "  import json\n",
    "  !wget \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "  class_idx = json.load(open(\"imagenet_class_index.json\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnvXSmtyLAgz"
   },
   "outputs": [],
   "source": [
    "# Thanks to https://discuss.pytorch.org/t/imagenet-classes/4923/2\n",
    "def load_imagenet_classes():\n",
    "    classes_json = urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json').read()\n",
    "    classes = json.loads(classes_json)\n",
    "    \n",
    "    # TODO: Process it to return dict of class index to name\n",
    "    return { int(k): v[-1] for k, v in classes.items()}\n",
    "\n",
    "  \n",
    "\n",
    "def print_labels(class_idx, out, num_labels=10):\n",
    "  idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "  res=[]\n",
    "  #print(\"top10\", out[0].sort(descending = True, dim=-1)[1].detach().numpy()[:num_labels])\n",
    "  for idx in out[0].sort(descending = True, dim=0)[1].detach().numpy()[:num_labels]:\n",
    "    res.append(idx2label[idx])\n",
    "  return res\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.cuda()\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "#np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "ind10 = indices[:10]\n",
    "\n",
    "sampler10 = SubsetRandomSampler(ind10)\n",
    "\n",
    "\n",
    "first_try_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, \n",
    "                                           sampler=sampler10)\n",
    "#print(list(first_try_loader, val_loader))\n",
    "labels=[]\n",
    "\n",
    "\n",
    "for i, (x, y,_) in enumerate(first_try_loader):\n",
    "  x_gpu = x.to(device)\n",
    "  with torch.no_grad():\n",
    "    prediction = model(x_gpu) \n",
    "    #print(prediction[0].sort()[0])\n",
    "  #plt.title(\"Resnet 18 labels:\\n %s\" % print_labels(class_idx, prediction, 3))\n",
    "  #plt.imshow(train_dataset[ind10[i]])\n",
    "  labels.append(print_labels(class_idx, prediction.cpu(), 3))\n",
    "  \n",
    "visualize_samples(dataset, ind10, \"resnet18\", 10, labels)\n",
    "#print(labels)\n",
    "#visualize_samples(orig_dataset, indices, \"Samples\")\n",
    "# TODO: Run this model on 10 random images of your dataset and visualize what it predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6a-3a1ZFGEw_"
   },
   "source": [
    "# Перенос обучения (transfer learning) - тренировать только последний слой\n",
    "\n",
    "Существует несколько вариантов переноса обучения, мы попробуем основные.  \n",
    "Первый вариант - заменить последний слой на новый и тренировать только его, заморозив остальные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCWMUWmr7t5g"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model_st1 = models.resnet18(pretrained=True)\n",
    "# TODO: Freeze all the layers of this model and add a new output layer\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "for param in model_st1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_st1.fc.in_features\n",
    "model_st1.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_st1 = model_st1.to(device)\n",
    "\n",
    "parameters = model_st1.fc.parameters()   # Fill the right thing here!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
    "loss_history, train_history, val_history = train_model(model_st1, train_loader, val_loader, loss, optimizer, None, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dDH4WfaB2Il"
   },
   "source": [
    "# Перенос обучения (transfer learning) - тренировать всю модель\n",
    "\n",
    "Второй вариант - точно так же заменить последний слой на новый и обучать всю модель целиком.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ss0jilyvuOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average loss: 0.428577, Train accuracy: 0.799891, Val accuracy: 0.900000, Val F1: 0.795014\n",
      "Epoch: 1, Average loss: 0.357152, Train accuracy: 0.840348, Val accuracy: 0.918478, Val F1: 0.817539\n",
      "Epoch: 2, Average loss: 0.330430, Train accuracy: 0.854195, Val accuracy: 0.920652, Val F1: 0.824536\n",
      "Epoch: 3, Average loss: 0.331509, Train accuracy: 0.855281, Val accuracy: 0.919565, Val F1: 0.831884\n",
      "Epoch: 4, Average loss: 0.321927, Train accuracy: 0.856096, Val accuracy: 0.913043, Val F1: 0.835260\n",
      "Epoch: 5, Average loss: 0.320681, Train accuracy: 0.861797, Val accuracy: 0.920652, Val F1: 0.840708\n",
      "Epoch: 6, Average loss: 0.308599, Train accuracy: 0.874016, Val accuracy: 0.926087, Val F1: 0.853767\n",
      "Epoch: 7, Average loss: 0.322082, Train accuracy: 0.856096, Val accuracy: 0.927174, Val F1: 0.834553\n",
      "Epoch: 8, Average loss: 0.310126, Train accuracy: 0.864513, Val accuracy: 0.927174, Val F1: 0.830904\n",
      "Epoch: 9, Average loss: 0.314188, Train accuracy: 0.863427, Val accuracy: 0.925000, Val F1: 0.850370\n",
      "Epoch: 11, Average loss: 0.327398, Train accuracy: 0.854738, Val accuracy: 0.925000, Val F1: 0.854573\n",
      "Epoch: 12, Average loss: 0.310689, Train accuracy: 0.867228, Val accuracy: 0.920652, Val F1: 0.851632\n",
      "Epoch: 13, Average loss: 0.305671, Train accuracy: 0.874559, Val accuracy: 0.910870, Val F1: 0.840876\n",
      "Epoch: 14, Average loss: 0.309522, Train accuracy: 0.866956, Val accuracy: 0.936957, Val F1: 0.846498\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model_st2 = models.resnet18(pretrained=True)\n",
    "# TODO: Add a new output layer and train the whole model\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "num_ftrs = model_st2.fc.in_features\n",
    "model_st2.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_st2 = model_st2.to(device)\n",
    "\n",
    "parameters = model_st2.fc.parameters()   # Fill the right thing here!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(parameters, lr=0.001, weight_decay = 1e-4)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "#exp_lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 1, factor=0.2)\n",
    "\n",
    "loss_history, train_history, val_history = train_model(model_st2, train_loader, val_loader, \n",
    "                                                       loss, optimizer, exp_lr_scheduler, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average loss: 0.303269, Train accuracy: 0.877545, Val accuracy: 0.938043, Val F1: 0.851632\n",
      "Epoch: 1, Average loss: 0.305129, Train accuracy: 0.869400, Val accuracy: 0.935870, Val F1: 0.864865\n",
      "Epoch: 2, Average loss: 0.307165, Train accuracy: 0.868857, Val accuracy: 0.927174, Val F1: 0.852941\n",
      "Epoch: 3, Average loss: 0.301009, Train accuracy: 0.868585, Val accuracy: 0.931522, Val F1: 0.847858\n",
      "Epoch: 4, Average loss: 0.312907, Train accuracy: 0.868585, Val accuracy: 0.900000, Val F1: 0.844444\n",
      "Epoch: 5, Average loss: 0.307143, Train accuracy: 0.869128, Val accuracy: 0.932609, Val F1: 0.840708\n",
      "Epoch: 6, Average loss: 0.312608, Train accuracy: 0.865056, Val accuracy: 0.910870, Val F1: 0.854599\n",
      "Epoch: 7, Average loss: 0.328310, Train accuracy: 0.866142, Val accuracy: 0.939130, Val F1: 0.847407\n",
      "Epoch: 8, Average loss: 0.298355, Train accuracy: 0.877817, Val accuracy: 0.932609, Val F1: 0.854573\n",
      "Epoch: 9, Average loss: 0.308768, Train accuracy: 0.872658, Val accuracy: 0.940217, Val F1: 0.857571\n",
      "Epoch: 10, Average loss: 0.311167, Train accuracy: 0.870486, Val accuracy: 0.920652, Val F1: 0.849112\n",
      "Epoch: 11, Average loss: 0.298351, Train accuracy: 0.869128, Val accuracy: 0.927174, Val F1: 0.862745\n",
      "Epoch: 12, Average loss: 0.298790, Train accuracy: 0.870486, Val accuracy: 0.933696, Val F1: 0.858841\n",
      "Epoch: 13, Average loss: 0.309449, Train accuracy: 0.871029, Val accuracy: 0.927174, Val F1: 0.851252\n",
      "Epoch: 14, Average loss: 0.307467, Train accuracy: 0.874287, Val accuracy: 0.926087, Val F1: 0.853731\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, val_history = train_model(model_st2, train_loader, val_loader, \n",
    "                                                       loss, optimizer, exp_lr_scheduler, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_st2.state_dict(), \"model_st2.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meQt_vDCs9cc"
   },
   "source": [
    "# Перенос обучения (transfer learning) - разные скорости обучения для разных слоев\n",
    "\n",
    "И наконец последний вариант, который мы рассмотрим - использовать разные скорости обучения для новых и старых слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evro9ksXGs9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average loss: 0.504205, Train accuracy: 0.805593, Val accuracy: 0.905435, Val F1: 0.835366\n",
      "Epoch: 1, Average loss: 0.449793, Train accuracy: 0.822156, Val accuracy: 0.815217, Val F1: 0.829047\n",
      "Epoch: 2, Average loss: 0.452058, Train accuracy: 0.838718, Val accuracy: 0.881522, Val F1: 0.841782\n",
      "Epoch: 3, Average loss: 0.397116, Train accuracy: 0.850122, Val accuracy: 0.920652, Val F1: 0.860681\n",
      "Epoch: 4, Average loss: 0.412344, Train accuracy: 0.845235, Val accuracy: 0.915217, Val F1: 0.858476\n",
      "Epoch: 5, Average loss: 0.454904, Train accuracy: 0.835189, Val accuracy: 0.918478, Val F1: 0.852615\n",
      "Epoch: 6, Average loss: 0.468482, Train accuracy: 0.836818, Val accuracy: 0.666304, Val F1: 0.852255\n",
      "Epoch: 7, Average loss: 0.798230, Train accuracy: 0.806136, Val accuracy: 0.904348, Val F1: 0.839744\n",
      "Epoch: 8, Average loss: 0.471833, Train accuracy: 0.845235, Val accuracy: 0.909783, Val F1: 0.840764\n",
      "Epoch: 9, Average loss: 0.402046, Train accuracy: 0.852023, Val accuracy: 0.816304, Val F1: 0.839117\n",
      "Epoch: 10, Average loss: 0.357107, Train accuracy: 0.862612, Val accuracy: 0.918478, Val F1: 0.831530\n",
      "Epoch: 11, Average loss: 0.338692, Train accuracy: 0.867228, Val accuracy: 0.917391, Val F1: 0.864353\n",
      "Epoch: 12, Average loss: 0.324956, Train accuracy: 0.872387, Val accuracy: 0.926087, Val F1: 0.858491\n",
      "Epoch: 13, Average loss: 0.328467, Train accuracy: 0.875916, Val accuracy: 0.925000, Val F1: 0.873239\n",
      "Epoch: 14, Average loss: 0.333724, Train accuracy: 0.861526, Val accuracy: 0.913043, Val F1: 0.835913\n",
      "Epoch: 15, Average loss: 0.318603, Train accuracy: 0.871844, Val accuracy: 0.909783, Val F1: 0.849145\n",
      "Epoch: 17, Average loss: 0.299036, Train accuracy: 0.873473, Val accuracy: 0.919565, Val F1: 0.859813\n",
      "Epoch: 18, Average loss: 0.317915, Train accuracy: 0.870486, Val accuracy: 0.909783, Val F1: 0.828829\n",
      "Epoch: 19, Average loss: 0.309721, Train accuracy: 0.859897, Val accuracy: 0.914130, Val F1: 0.837920\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model_st3 = models.resnet18(pretrained=True)\n",
    "# TODO: Add a new output layer\n",
    "# Train new layer with learning speed 0.001 and old layers with 0.0001\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "num_ftrs = model_st3.fc.in_features\n",
    "model_st3.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_st3 = model_st3.to(device)\n",
    "\n",
    "#parameters = model_st3.fc.parameters()   # Fill the right thing here!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "                {'params': model_st3.fc.parameters(), 'lr': 1e-2, 'weight_decay' : 1e-3}\n",
    "            ], lr=1e-3, weight_decay = 1e-4)\n",
    "exp_lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 3, factor=0.1)\n",
    "loss_history, train_history, val_history = train_model(model_st3, train_loader, val_loader, \n",
    "                                                       loss, optimizer, exp_lr_scheduler, 20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXQP-gXV90qT"
   },
   "source": [
    "# Визуализируем метрики и ошибки модели\n",
    "\n",
    "Попробуем посмотреть, где модель ошибается - визуализируем ложные срабатывания (false positives) и ложноотрицательные срабатывания (false negatives).\n",
    "\n",
    "Для этого мы прогоним модель через все примеры и сравним ее с истинными метками (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieEzZUglJAUB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions, gt = evaluate_model(model_st2, train_dataset, val_indices)\n",
    "print(predictions, \"\\n\", gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0bcioK6JBDK"
   },
   "source": [
    "И теперь можно визуализировать false positives и false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMmaPfdeKk9H"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute indices of the false positives on the validation set.\n",
    "# Note those have to be indices of the original dataset\n",
    "diff = (np.array(predictions) - np.array(gt))\n",
    "\n",
    "false_positive_indices = np.array(val_indices)[diff == 1]\n",
    "visualize_samples(orig_dataset, false_positive_indices, \"False positives\")\n",
    "\n",
    "# TODO: Compute indices of the false negatives on the validation set.\n",
    "# Note those have to be indices of the original dataset\n",
    "false_negatives_indices = np.array(val_indices)[diff == -1]\n",
    "visualize_samples(orig_dataset, false_negatives_indices, \"False negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoDeVjN4HZSV"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1 = binary_classification_metrics(predictions, gt)\n",
    "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (f1, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_O9qiYySvuj"
   },
   "source": [
    "# Что будет в конце вы уже поняли\n",
    "\n",
    "Натренируйте лучшую модель на основе `resnet18`, меняя только процесс тренировки.\n",
    "Выбирайте лучшую модель по F1 score.\n",
    "\n",
    "Как всегда, не забываем:\n",
    "- побольше агментаций!\n",
    "- перебор гиперпараметров\n",
    "- различные оптимизаторы\n",
    "- какие слои тюнить\n",
    "- learning rate annealing\n",
    "- на какой эпохе останавливаться\n",
    "\n",
    "Наша цель - довести F1 score на validation set до значения, большего **0.93**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6mhfdQ9K-N3"
   },
   "outputs": [],
   "source": [
    "# TODO: Train your best model!\n",
    "best_model = model_st3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6xExdw8JB1l"
   },
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set!\n",
    "predictions1, ground_truth = evaluate_model(model_st3, val_dataset, val_indices)\n",
    "precision, recall, f1 = binary_classification_metrics(predictions1, ground_truth)\n",
    "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (f1, precision, recall))\n",
    "\n",
    "# TODO: Visualize training curve for the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaVK7hok90qy"
   },
   "source": [
    "## Визуализируйте ошибки лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFUeNOm1VACr"
   },
   "outputs": [],
   "source": [
    "# TODO Visualize false positives and false negatives of the best model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVT6gGJp90q4"
   },
   "source": [
    "# Необязательное задание с большой звездочкой\n",
    "\n",
    "Поучавствуйте в Kaggle In-Class Hot Dog Recognition Challenge!  \n",
    "Это соревнование сделано специально для курса и в нем учавствуют только те, кто проходит курс.\n",
    "\n",
    "В нем участники соревнуются в качестве натренированных моделей, загружая на сайт предсказания своих моделей на тестовой выборке. Разметка тестовой выборке участникам недоступна.\n",
    "Более подробно о правилах соревнования ниже.\n",
    "\n",
    "Те, кто проходят курс лично, за высокое место в соревновании получат дополнительные баллы.\n",
    "\n",
    "Здесь уже можно использовать и другие базовые архитектуры кроме `resnet18`, и ансамбли, и другие трюки тренировки моделей.\n",
    "\n",
    "Вот ссылка на соревнование:\n",
    "https://www.kaggle.com/c/hotdogornot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSkDngjo90q5"
   },
   "outputs": [],
   "source": [
    "image_id = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "for x,_,id_img in test_loader:\n",
    "    # TODO : Напишите код для предсказания меток (1 = есть хотдог, 0 = хотдога нет)\n",
    "    # Код должен возвратить список из id картинки и метку predictions\n",
    "    # image id - это название файла картинки, например '10000.jpg'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1R0jGUcO90q9"
   },
   "outputs": [],
   "source": [
    "# Так можно создать csv файл, чтобы затем загрузить его на kaggle\n",
    "# Ожидаемый формат csv-файла:\n",
    "# image_id,label\n",
    "# 10000.jpg,1\n",
    "# 10001.jpg,1\n",
    "# 10002.jpg,0\n",
    "# 10003.jpg,1\n",
    "# 10004.jpg,0\n",
    "\n",
    "with open('subm.csv', 'w') as submissionFile:\n",
    "    writer = csv.writer(submissionFile)\n",
    "    writer.writerow(['image_id', 'label'])\n",
    "    writer.writerows(zip(image_id,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqtGCil790rB"
   },
   "outputs": [],
   "source": [
    "# А так можно скачать файл с Google Colab\n",
    "files.download('subm.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svD8j_nC90rG"
   },
   "source": [
    "### Небольшое введение в Kaggle для тех, кто не слышал об этой платформе раньше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyeuy58r90rH"
   },
   "source": [
    "В основе своей Kaggle - это платформа для проведения соревнований по машинному обучению. Появилась она в 2010 и, пожалуй, стала самой популярной и известной из всех существующих площадок по машинному обучению. Надо сказать, что Kaggle - это не только соревнования, но и сообщество людей, увлеченных машинным обучением. А судя по Википедии, в 2017 году отметка зарегистрированных пользователей перевалила за миллион. Есть там и обучающие материалы, возможность задавать вопросы, делиться кодом и идеями - просто мечта. \n",
    "\n",
    "### Как проходят соревнования? \n",
    "Обычно участники скачивают данные для обучения моделей (train data), чтобы затем делать предсказания на тестовых данных (test data). Обучающая выборка содержит как сами данные, так и правильные метки (значения зависимой переменной), чтобы можно было обучить модель. Но тестовые данные ответа не содержат - и нашей целью является предсказание меток по имеющимся данным. Файл с ответами для каждого наблюдения из тестовой выборки загружается на Kaggle и оценивается в соответствии с выбранной метрикой соревнования, а результат является публичным и показывается в общей таблице (ее называют еще лидербордом - leaderboard) - чтобы появилось желание посоревноваться и создать еще более сильную модель. В \"настоящих\" соревнованиях, которые проходят на Kaggle, есть и денежное вознаграждение для тех участников, кто занимает первые места на лидерборде. Например, в [этом](https://www.kaggle.com/c/zillow-prize-1#description) соревновании, человек, занявший первое место, получил около 1 000 000 долларов. \n",
    "\n",
    "Тестовые данные делятся случайным образом в некоторой пропорции. И пока соревнование идет, на лидерборде показываются очки и рейтинг участников только по одной части (Public Leaderboard). А вот когда соревнование заканчивается, то рейтинг участников составляется по второй части тестовых данных (Private Leaderboard). И часто можно видеть, как люди занимавшие первые места на публичной части тестовых данных, оказываются далеко не первыми на закрытой части тестовых данных. Зачем это сделано? Есть несколько причин, но, пожалуй, самой фундаментальной является идея недообучения-переобучения. Всегда возможно, что наша модель настроилась на конкретную выборку, но как она поведет себя на тех данных, которые еще не видела? Разбиение тестовых данных на публичную и скрытую части сделано для того, чтобы отобрать модели, которые имеют большую обобщающую способность. Одним из лозунгов участников соревнований стал \"Доверяйте своей локальной кросс-валидации\" (Trust your CV!). Есть очень большой соблазн оценивать свою модель по публичной части лидерборда, но лучшей стратегией будет выбирать ту модель, которая дает лучшую метрику на кросс-валидации на обучающей выборке. \n",
    "\n",
    "В нашем соревновании публичная часть лидерборда составляет 30%, а скрытая 70%. Вы можете делать до двух попыток в день, а оцениваться попытки будут по F1-мере. Удачи и доверяйте своей локальной валидации! В конце соревнования у вас будет возможность выбрать 2 из всех совершенных попыток - лучшая из этих двух и будет засчитана вам на скрытой части тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRWJmGGN90rI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HotdogOrNot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
